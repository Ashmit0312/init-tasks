{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a7fe705-0818-437b-a506-969640197fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models, losses, optimizers\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Normalize\n",
    "x_train = tf.cast(x_train, tf.float32) / 255.0\n",
    "x_test = tf.cast(x_test, tf.float32) / 255.0\n",
    "\n",
    "# Flatten\n",
    "x_train = tf.reshape(x_train, (-1, 784))\n",
    "x_test = tf.reshape(x_test, (-1, 784))\n",
    "\n",
    "# Convert labels\n",
    "y_train = tf.cast(y_train, tf.int64)\n",
    "y_test = tf.cast(y_test, tf.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2144ba5-c017-4aff-9eef-c4c58d6632ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60000, 1, 784])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = tf.cast(x_train, tf.float32) / 255.0\n",
    "images = tf.expand_dims(images, axis=1)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4dbfc5-3976-44c8-8c6a-b459d663106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 784)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "batch_images = images[:batch_size]\n",
    "\n",
    "print(batch_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd93eac4-0e4f-40f7-9a0b-135b99817351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 784)\n"
     ]
    }
   ],
   "source": [
    "flattened_images = tf.reshape(batch_images, (batch_size, -1))\n",
    "\n",
    "print( flattened_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd1b4a5-56ab-467b-b689-6bd54bf7a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = 784\n",
    "output_neurons = 10  # Fashion-MNIST has 10 classes\n",
    "\n",
    "W = tf.Variable(tf.random.normal((output_neurons, input_features)))\n",
    "B = tf.Variable(tf.random.normal((output_neurons,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c341750f-dd6c-4cb4-a8dd-cfd340bc7c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = tf.matmul(flattened_images, tf.transpose(W)) + B\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3811a80a-9837-4941-badf-07c811600f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Shape (W): (10, 784)\n",
      "Gradient Shape (B): (10,)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    \n",
    "    Y = tf.matmul(flattened_images, tf.transpose(W)) + B\n",
    "    \n",
    "    # Dummy loss\n",
    "    loss = tf.reduce_mean(Y)\n",
    "\n",
    "# Compute gradients\n",
    "grad_W, grad_B = tape.gradient(loss, [W, B])\n",
    "\n",
    "print(\"Gradient Shape (W):\", grad_W.shape)\n",
    "print(\"Gradient Shape (B):\", grad_B.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "281ce5fd-d42f-4b14-87fd-17430ff838f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SimpleANN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "        self.out = tf.keras.layers.Dense(10)  # logits\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f76bc0-c127-47f8-b03e-36a3a818d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model = SimpleANN()\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c31cf069-d9f1-4913-99a6-591735ef8174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7903\n",
      "Epoch 2, Loss: 1.2053\n",
      "Epoch 3, Loss: 1.0087\n",
      "Epoch 4, Loss: 0.9150\n",
      "Epoch 5, Loss: 0.8470\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(60000).batch(batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    total_loss = 0\n",
    "    batches = 0\n",
    "    \n",
    "    for X_batch, y_batch in dataset:\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Use model() instead of model.call()\n",
    "            predictions = model(X_batch, training=True)\n",
    "            \n",
    "            loss = loss_fn(y_batch, predictions)\n",
    "        \n",
    "        # ðŸ”¥ Get ALL trainable variables automatically\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # ðŸ”¥ Manual Gradient Descent\n",
    "        for param, grad in zip(model.trainable_variables, grads):\n",
    "            param.assign_sub(learning_rate * grad)\n",
    "        \n",
    "        total_loss += loss.numpy()\n",
    "        batches += 1\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebe58444-5a02-456e-a557-9113ad2d35b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "    \n",
    "\n",
    "predicted_classes = tf.argmax(predictions, axis=1)\n",
    "        \n",
    "correct = tf.equal(predicted_classes, y_batch)\n",
    "total_correct += tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "total_samples += y_batch.shape[0]\n",
    "\n",
    "total_loss += loss.numpy()\n",
    "epoch_accuracy = total_correct / total_samples\n",
    "print(f\"Accuracy: {epoch_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c14f47f1-39b0-4c33-9d2b-fc85538765b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train[:100], y_train[:100])\n",
    ").batch(16)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test[:1000],y_test[:1000])).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2da5d076-1719-4125-aa56-b73c493e88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleANN()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf7a3462-371d-4e02-bf97-c8dde1ea1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "LR = 0.01\n",
    "\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "920ffb9d-0b63-4747-981a-03ba8a2d1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataset):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in dataset:\n",
    "        \n",
    "        logits = model(images, training=False)  # VERY important!\n",
    "        preds = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
    "        \n",
    "        correct += tf.reduce_sum(tf.cast(preds == labels, tf.int32))\n",
    "        total += labels.shape[0]\n",
    "    \n",
    "    return (correct / total).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4da3d68a-9351-427a-913e-37dd55e9ed8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n",
      "Train Loss: 2.3131 | Train Acc: 0.1500\n",
      "Val Loss: 2.1634 | Val Acc: 0.1930\n",
      "\n",
      "Epoch 2/30\n",
      "Train Loss: 2.0688 | Train Acc: 0.2600\n",
      "Val Loss: 2.0491 | Val Acc: 0.2440\n",
      "\n",
      "Epoch 3/30\n",
      "Train Loss: 1.9025 | Train Acc: 0.3900\n",
      "Val Loss: 1.9509 | Val Acc: 0.3440\n",
      "\n",
      "Epoch 4/30\n",
      "Train Loss: 1.7695 | Train Acc: 0.5100\n",
      "Val Loss: 1.8600 | Val Acc: 0.4050\n",
      "\n",
      "Epoch 5/30\n",
      "Train Loss: 1.6555 | Train Acc: 0.5400\n",
      "Val Loss: 1.7772 | Val Acc: 0.4330\n",
      "\n",
      "Epoch 6/30\n",
      "Train Loss: 1.5532 | Train Acc: 0.5900\n",
      "Val Loss: 1.7016 | Val Acc: 0.4790\n",
      "\n",
      "Epoch 7/30\n",
      "Train Loss: 1.4603 | Train Acc: 0.6900\n",
      "Val Loss: 1.6357 | Val Acc: 0.4960\n",
      "\n",
      "Epoch 8/30\n",
      "Train Loss: 1.3794 | Train Acc: 0.7300\n",
      "Val Loss: 1.5755 | Val Acc: 0.5230\n",
      "\n",
      "Epoch 9/30\n",
      "Train Loss: 1.3059 | Train Acc: 0.7500\n",
      "Val Loss: 1.5214 | Val Acc: 0.5470\n",
      "\n",
      "Epoch 10/30\n",
      "Train Loss: 1.2392 | Train Acc: 0.7700\n",
      "Val Loss: 1.4699 | Val Acc: 0.5700\n",
      "\n",
      "Epoch 11/30\n",
      "Train Loss: 1.1766 | Train Acc: 0.7900\n",
      "Val Loss: 1.4230 | Val Acc: 0.5900\n",
      "\n",
      "Epoch 12/30\n",
      "Train Loss: 1.1212 | Train Acc: 0.8100\n",
      "Val Loss: 1.3813 | Val Acc: 0.6090\n",
      "\n",
      "Epoch 13/30\n",
      "Train Loss: 1.0688 | Train Acc: 0.8400\n",
      "Val Loss: 1.3435 | Val Acc: 0.6190\n",
      "\n",
      "Epoch 14/30\n",
      "Train Loss: 1.0210 | Train Acc: 0.8500\n",
      "Val Loss: 1.3089 | Val Acc: 0.6280\n",
      "\n",
      "Epoch 15/30\n",
      "Train Loss: 0.9762 | Train Acc: 0.8500\n",
      "Val Loss: 1.2755 | Val Acc: 0.6380\n",
      "\n",
      "Epoch 16/30\n",
      "Train Loss: 0.9348 | Train Acc: 0.8600\n",
      "Val Loss: 1.2458 | Val Acc: 0.6430\n",
      "\n",
      "Epoch 17/30\n",
      "Train Loss: 0.8956 | Train Acc: 0.8600\n",
      "Val Loss: 1.2173 | Val Acc: 0.6460\n",
      "\n",
      "Epoch 18/30\n",
      "Train Loss: 0.8596 | Train Acc: 0.8700\n",
      "Val Loss: 1.1920 | Val Acc: 0.6490\n",
      "\n",
      "Epoch 19/30\n",
      "Train Loss: 0.8251 | Train Acc: 0.8800\n",
      "Val Loss: 1.1683 | Val Acc: 0.6550\n",
      "\n",
      "Epoch 20/30\n",
      "Train Loss: 0.7939 | Train Acc: 0.8800\n",
      "Val Loss: 1.1477 | Val Acc: 0.6550\n",
      "\n",
      "Epoch 21/30\n",
      "Train Loss: 0.7637 | Train Acc: 0.8800\n",
      "Val Loss: 1.1294 | Val Acc: 0.6610\n",
      "\n",
      "Epoch 22/30\n",
      "Train Loss: 0.7357 | Train Acc: 0.8900\n",
      "Val Loss: 1.1111 | Val Acc: 0.6600\n",
      "\n",
      "Epoch 23/30\n",
      "Train Loss: 0.7093 | Train Acc: 0.8900\n",
      "Val Loss: 1.0953 | Val Acc: 0.6620\n",
      "\n",
      "Epoch 24/30\n",
      "Train Loss: 0.6847 | Train Acc: 0.9000\n",
      "Val Loss: 1.0790 | Val Acc: 0.6710\n",
      "\n",
      "Epoch 25/30\n",
      "Train Loss: 0.6610 | Train Acc: 0.9000\n",
      "Val Loss: 1.0653 | Val Acc: 0.6740\n",
      "\n",
      "Epoch 26/30\n",
      "Train Loss: 0.6380 | Train Acc: 0.9000\n",
      "Val Loss: 1.0522 | Val Acc: 0.6750\n",
      "\n",
      "Epoch 27/30\n",
      "Train Loss: 0.6168 | Train Acc: 0.9000\n",
      "Val Loss: 1.0401 | Val Acc: 0.6770\n",
      "\n",
      "Epoch 28/30\n",
      "Train Loss: 0.5967 | Train Acc: 0.9000\n",
      "Val Loss: 1.0281 | Val Acc: 0.6800\n",
      "\n",
      "Epoch 29/30\n",
      "Train Loss: 0.5776 | Train Acc: 0.9100\n",
      "Val Loss: 1.0180 | Val Acc: 0.6820\n",
      "\n",
      "Epoch 30/30\n",
      "Train Loss: 0.5596 | Train Acc: 0.9100\n",
      "Val Loss: 1.0080 | Val Acc: 0.6850\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    total_loss = 0\n",
    "    batches = 0\n",
    "\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            logits = model(images, training=True)\n",
    "            loss = loss_fn(labels, logits)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "        for param, grad in zip(model.trainable_variables, grads):\n",
    "            param.assign_sub(LR * grad)\n",
    "\n",
    "        total_loss += loss.numpy()\n",
    "        batches += 1\n",
    "\n",
    "    avg_train_loss = total_loss / batches\n",
    "    \n",
    "    \n",
    "    val_loss_total = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for images, labels in val_ds:\n",
    "        \n",
    "        logits = model(images, training=False)\n",
    "        loss = loss_fn(labels, logits)\n",
    "        \n",
    "        val_loss_total += loss.numpy()\n",
    "        val_batches += 1\n",
    "\n",
    "    avg_val_loss = val_loss_total / val_batches \n",
    "    \n",
    "    train_acc = compute_accuracy(model, train_ds)\n",
    "    val_acc = compute_accuracy(model, val_ds)\n",
    "    \n",
    "    train_loss_history.append(avg_train_loss)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    \n",
    "    train_acc_history.append(train_acc)\n",
    "    val_acc_history.append(val_acc)\n",
    "\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4cd9371-e386-4cfd-9013-7f1c94b43d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([9 0 0 3 0 2 7 2 5 5], shape=(10,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    " for images, labels in train_ds:\n",
    "    print(labels[:10])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "798e5454-7ee9-4d42-b71d-1b59be5fec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(32, 3, activation='relu')\n",
    "        self.pool1 = layers.MaxPooling2D()\n",
    "        self.conv2 = layers.Conv2D(64, 3, activation='relu')\n",
    "        self.pool2 = layers.MaxPooling2D()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc1 = layers.Dense(128, activation='relu')\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "        self.fc2 = layers.Dense(10)  # logits\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.fc2(x)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e2b2b01-9b3b-4d87-825b-da76f220ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LR = 0.001\n",
    "\n",
    "model_cnn = SimpleCNN()\n",
    "loss_fn = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = optimizers.Adam(learning_rate=LR)\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a34aec44-b024-45d3-aac7-f4633c3b9da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vall_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7af25ed3-4b0f-4b4d-8c5f-74fee02add1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Train Loss: 0.8305 | Train Acc: 0.7605\n",
      "Val Loss: 0.7548 | Val Acc: 0.7497\n",
      "\n",
      "Epoch 2/5\n",
      "Train Loss: 0.6921 | Train Acc: 0.7855\n",
      "Val Loss: 0.6681 | Val Acc: 0.7755\n",
      "\n",
      "Epoch 3/5\n",
      "Train Loss: 0.6259 | Train Acc: 0.8015\n",
      "Val Loss: 0.6203 | Val Acc: 0.7925\n",
      "\n",
      "Epoch 4/5\n",
      "Train Loss: 0.5842 | Train Acc: 0.8133\n",
      "Val Loss: 0.5858 | Val Acc: 0.8023\n",
      "\n",
      "Epoch 5/5\n",
      "Train Loss: 0.5549 | Train Acc: 0.8204\n",
      "Val Loss: 0.5628 | Val Acc: 0.8102\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    total_loss = 0\n",
    "    batches = 0\n",
    "\n",
    "    \n",
    "    for images, labels in dataset:\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            logits = model(images, training=True)\n",
    "            loss = loss_fn(labels, logits)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "        for param, grad in zip(model.trainable_variables, grads):\n",
    "            param.assign_sub(LR * grad)\n",
    "\n",
    "        total_loss += loss.numpy()\n",
    "        batches += 1\n",
    "\n",
    "    avg_train_loss = total_loss / batches\n",
    "    \n",
    "    \n",
    "    val_loss_total = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for images, labels in vall_ds:\n",
    "        \n",
    "        logits = model(images, training=False)\n",
    "        loss = loss_fn(labels, logits)\n",
    "        \n",
    "        val_loss_total += loss.numpy()\n",
    "        val_batches += 1\n",
    "\n",
    "    avg_val_loss = val_loss_total / val_batches \n",
    "    \n",
    "    train_acc = compute_accuracy(model, dataset)\n",
    "    val_acc = compute_accuracy(model, vall_ds)\n",
    "    \n",
    "    train_loss_history.append(avg_train_loss)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    \n",
    "    train_acc_history.append(train_acc)\n",
    "    val_acc_history.append(val_acc)\n",
    "\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bbb3fe-8e65-4b28-8208-1f9766c943a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
